{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "result = load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "if result:\n",
    "    print(\"Variables de entorno cargadas exitosamente.\")\n",
    "else:\n",
    "    print(\"No se pudo cargar el archivo .env.\")\n",
    "\n",
    "    # Crear el archivo .env con las variables deseadas si no existe\n",
    "    with open(\".env\", \"w\") as f:\n",
    "        f.write(f\"OPENAI_API_KEY=MY_OPENAI_API_KEY\\n\")\n",
    "        f.write(f\"PINECONE_API_KEY=MY_PINECONE_API_KEY\\n\")\n",
    "        f.write(f\"PINECONE_ENVIRONMENT=MY_PINECONE_ENVIRONMENT\\n\")\n",
    "\n",
    "    print(\"Se creó el archivo .env con las variables iniciales.\")\n",
    "    # Vuelve a cargar las variables de entorno después de crear el archivo\n",
    "    load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\") or \"MY_OPENAI_API_KEY\"\n",
    "os.environ['PINECONE_API_KEY'] =  os.getenv(\"PINECONE_API_KEY\") or \"MY_PINECONE_API_KEY\"\n",
    "os.environ[\"PINECONE_ENVIRONMENT\"] = os.getenv(\"PINECONE_ENVIRONMENT\") or \"MY_PINECONE_ENVIRONMENT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Models (GPT-3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                  model_name='gpt-3.5-turbo-1106', \n",
    "                  temperature=0.5, max_tokens = 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hacemos el modelo al que le vamos a pedir que nos responda, dandole la api_key, el nombre del modelo, la temperatura y el maximo de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",

    "    ('system', 'Responde segun el idioma que te escriban en el prompt. Ejemplo: input en Espanol: Hola como estas? Respondes en espanol, input en Ingles: Hello how are you? respondes en ingles'),#contexto del modelo,\n",
    "    ('user', '{input}')#mensaje\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos una plantilla de prompt donde en system le damos un contexto al modelo para que pueda responder, en user agregaremos el prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | gpt3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la cadena uniendo el prompt y el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain.invoke({\"input\":\"dime un chiste corto  nuevo sobre montanas\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser =  StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | gpt3 | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder ver mejor el mensaje de respuesta del modelo, creamos una cadena agregandole la funcion de output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    salida = chain.invoke({\"input\":\"Hola, buenos dias, sabes que es alvearium?\"})\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con la llamada a openai, aqui podemos ver la cantidad de tokens utolizados y el precio total de ellos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto podemos notar que solo es un chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuenta de cuantos tokens puede tener una entrada y una salidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "\n",
    "texts = text_splitter.split_text(salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otra manera de contar tokens con Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos una funcion para contar los tokens\n",
    "\n",
    "def num_tokens_from_strings(string, encoding_name=str) -> str:\n",
    "    \"Retorna el numero de tokens\"\n",
    "    encoding =  tiktoken.encoding_for_model(encoding_name) #Encoding_for_model es para encodear especificamente bajo el modelo que se este utilizando\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    print(f\"El numero de tokens de salida es: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_strings(salida, \"gpt-3.5-turbo-1106\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-env",
   "language": "python",
   "name": "chatbot-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
